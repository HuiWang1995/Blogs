>应用服务器：8c 16g
>限制： TPS需要高于100，平均CPU需要低于80%，成功率需高于99.99%
# 初始

以余额变动管理为例，在2021/11/17，200并发的压测结果来看：

|TPS|平均延迟|平均CPU|平均内存|
|:----|:----|:----|:----|
|41.03|2130.88|35.13%|40.84%|

TPS很低，平均延迟也很长，CPU利用率也很低。

# 第一阶段

黎金丰通过调节了以下参数（具体调参过程、依据不明）：

|参数KEY|参数VALUE|
|:----|:----|
|server.tomcat.max-connections|6000|
|server.tomcat.min-spare-threads|50|
|server.tomcat.max-threads|500|
|threadPool.config.capacity|200|
|threadPool.config.corePoolSzie|35|

于2021/11/26，200并发的压测结果来看：

|TPS|平均延迟|平均CPU|平均内存|
|:----|:----|:----|:----|
|63|1715.17|65.96%|51.77%|

通过大量倍增线程池中线程数量，连接池的上限，有效的提高了CPU使用率，但是对于延迟的降低有限，同样TPS也是十分的低。

同日，300并发下的压测结果来看：

|TPS|平均延迟|平均CPU|平均内存|
|:----|:----|:----|:----|
|99.6|1760.3|75.51%|46.48%|

对比200并发，CPU使用率有所上升，延迟基本未变化，TPS成等比上升，较为平稳。

# 第二阶段

王灵辉调节了以下参数：

|参数KEY|参数VALUE|
|:----|:----|
|threadPool.config.corePoolSize|2|
|threadPool.config.maxPoolSize|100|
|threadPool.config.idle|3|
|threadPool.config.capacity|512|
|threadPool.config.aql.corePoolSize|10|
|threadPool.config.aql.maxPoolSize|128|
|threadPool.config.aql.idle|3|
|threadPool.config.aql.capacity|2048|
|spring.redis.redisson.connectionPoolSize|4|
|spring.redis.redisson.connectionMinimumIdleSize|1|
|spring.redis.redisson.masterConnectionPoolSize|4|
|spring.redis.redisson.masterConnectionMinimumIdleSize|1|
|spring.redis.redisson.nettyThreads|8|
|spring.redis.redisson.slaveConnectionPoolSize|4|
|spring.redis.redisson.slaveConnectionMinimumIdleSize|1|
|spring.redis.redisson.subscriptionsPerConnection|5|
|spring.redis.redisson.slaveSubscriptionConnectionPoolSize|4|
|spring.redis.redisson.slaveSubscriptionConnectionMinimumIdleSize|1|
|spring.redis.redisson.subscriptionConnectionMinimumIdleSize|1|
|spring.redis.redisson.threads|8|
|spring.redis.redisson.subscriptionConnectionPoolSize|4|
|server.tomcat.min-spare-threads|16|
|server.tomcat.max-threads|32|
|server.tomcat.max-connections|512|
|server.tomcat.accept-count|2048|
|feign.httpclient.keepAlive|TRUE|
|feign.httpclient.maxConnections|256|
|feign.OkHttp.readTimeout|3000|
|feign.OkHttp.connectTimeout|20000|
|feign.OkHttp.writeTimeout|20000|

**调参主要判断依据**：

* 虚拟机线程运行状态
* 线程池队列等待数
* 连接池空闲连接数
**调参主要模块**：

1. 主线程池
主线程池基本任务数不多，大量削减其核心线程数（35->2），减少无用驻留线程。

2. AQL线程池
此线程主要用于AQL解析，曾与主线程使用同一套配置。而AQL的解析为CPU密集型，当其核心线程数在16时，200并发下基本全在运行状态，此时应用压测服务器的平均CPU为85%。为**降低CPU使用率**，减少其核心线程数到10个，并增大其阻塞队列容量。*基本使其成为短板来降低CPU*。

3. redis线程池、连接池
redis在整个接口中的时间比重很低，通过多次调整，发现其个位数的线程即能满足性能需求。

4. tomcat
tomcat作为面对并发的第一线战士，对其的连接数上限不能吝啬，否则容易导致成功率下降。将其最小线程数从50调整为16，最大线程数从500调整为32。

5. feign
feign组件使用okhttp作为底层http客户端，主要作用为请求公共服务中心职位信息、机构信息以及查询apache kylin数据。将其调整为keepAlive，可以有效减少建立TCP连接的开销。同时增加最大连接数，保证内网中能够建立足够多稳定的连接。另外，Feign的线程池无法调整，为框架自建。

>星云对OkHttpClient进行了注入，无法使用约定的ConnectionPool的配置项进行配置。需要自行注入。

于2021/12/15，200并发的压测结果来看：

|TPS|平均延迟|平均CPU|平均内存|
|:----|:----|:----|:----|
|311|636.37|63.13%|48.75%|

相比第一阶段的调参，大量减少了线程数，但是TPS从63上升到了311，延迟从1715.17下降到了636.37，CPU从65.96%变成63.13%。

同日，300并发下的压测结果来看：

|TPS|平均延迟|平均CPU|平均内存|
|:----|:----|:----|:----|
|300.85|981.42|62.98%|57.71%|

相比第一阶段的调参，大量减少了线程数，但是TPS从99.6上升到了300.85，延迟从1760.3下降到了981.42，CPU从75.51%变成62.98%，内存从46.48%变为了57.71%（应该是调整了-Xmx8g，原先为6g，JVM可用内存增加导致的，正常现象）。

同日500并发下的压测结果来看：

|TPS|平均延迟|平均CPU|平均内存|
|:----|:----|:----|:----|
|290.99|1692.53|60.87%|57.37%|

相比同配置300并发下的结果，TPS略有下降，延迟从981.42变为了1692.53，同时日志可以观测到AQL线程池队列与Tomcat线程池队列开始有较深堆积（500个）。CPU以及内存持平。且在500并发下，出现了错误数，为连接超时导致，整体成功率99.99%+。

# 总结

通过两个阶段的调优，可以判断此工程为CPU密集型，短板在于CPU。曾在压测中实验过，将AQL线程池核心线程数调大，CPU可以稳定到90%左右，使得TPS达到400+。假设在之后生产面临性能瓶颈时，可以考虑通过增加虚拟机核心数来提高并发服务能力。

